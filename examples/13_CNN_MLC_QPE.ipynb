{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network for QPE (2) \n",
    "## Multi-Label Classification\n",
    "\n",
    "In the previous example, we see tha CNN-based regression can do well in *correlation*, but the regression output is very unstable. In multiple trials, the center and spread shift randomly and hence this approach seems not suitable for forecasting.\n",
    "\n",
    "An alternative is to train a similar CNN for multi-label or binary classification, and then use the layers before the final output as dimension reduction filters and send to a more stable regressor such as GLM or SVM.\n",
    "\n",
    "Hence we will test classification based CNN in this example.\n",
    "\n",
    "\n",
    "### [Appendix] Categorize Continuous Variables\n",
    "\n",
    "In `numpy` there are two functions to convert continuous variables to cateogry ones: [`numpy.digitize()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.digitize.html) and [`numpy.histogram()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html), but they yeld different results.\n",
    "\n",
    "While [`numpy.histogram()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html) give categories only between the specified bins, [`numpy.digitize()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.digitize.html) gives extra category for values outside of the largest values in bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 51,  38, 155, 256], dtype=int64), array([  0,   5,  10,  30, 200]))\n",
      "1     51\n",
      "2     38\n",
      "3    155\n",
      "4    256\n",
      "dtype: int64\n",
      "[ 51  38 155 256]\n",
      "[0, 5, 10, 30, 200]\n",
      "[0, 5, 10, 30]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This block is for testing the behavior of `numpy.histogram` and `numpy.digitize`.\n",
    "# More detailed discussion can be found at: https://github.com/numpy/numpy/issues/9208\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ybins = [0, 5, 10, 30, 200]\n",
    "a = np.abs(np.random.randn(500)*50)\n",
    "# numpy.histogram\n",
    "ha = np.histogram(a, bins=ybins)\n",
    "print(ha)\n",
    "plt.hist(a, bins=ybins)\n",
    "# numpy.digitize\n",
    "da = np.digitize(a, ybins)\n",
    "dac = np.array(pd.Series(da).value_counts().sort_index())\n",
    "print(pd.Series(da).value_counts().sort_index())\n",
    "print(dac)\n",
    "print(ybins)\n",
    "print(ybins[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results shown above, when we use `numpy.digitize()` to categorize a continuous variable, we need to specify the number of categories in order to avoid empty bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, logging, argparse, glob, h5py, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Flatten, Activation\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.utils import normalize, to_categorical\n",
    "\n",
    "# Parameters\n",
    "nLayer = 6                      # 6 10-min dbz for an hour\n",
    "nY = 275                        # y-dimension of dbz data\n",
    "nX = 162                        # x-dimension of dbz data\n",
    "batchSize = 128                 # Batch size for training / testing\n",
    "prec_bins=[0, 0.5, 10, 15, 30, 1000]\n",
    "yseg_stat = [0.5, 8, 13, 29]    # 40-year statistics of hourly precipitation of trace, 90%, 95%, and 99% percentile\n",
    "yseg = [0.5, 10, 15, 30]        # Actual segmentation for precipitation\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "# Functions\n",
    "#-----------------------------------------------------------------------\n",
    "# Load input/output data for model\n",
    "def loadIOTab(srcx, srcy, dropna=False):\n",
    "    import pandas as pd\n",
    "    # Read raw input and output\n",
    "    #logging.info(\"Reading input X from: \"+ srcx)\n",
    "    print(\"Reading input X from: \"+ srcx)\n",
    "    xfiles = []\n",
    "    for root, dirs, files in os.walk(srcx): \n",
    "        for fn in files: \n",
    "            if fn.endswith('.npy'): \n",
    "                 xfiles.append({'date':fn.replace('.npy',''), 'xuri':os.path.join(root, fn)})\n",
    "    xfiles = pd.DataFrame(xfiles)\n",
    "    print(\"... read input size: \"+str(xfiles.shape))\n",
    "    #logging.info(\"Reading output Y from: \"+ srcy)\n",
    "    print(\"Reading output Y from: \"+ srcy)\n",
    "    yraw = pd.read_csv(srcy, encoding='utf-8')\n",
    "    yraw['date'] = yraw['date'].apply(str)\n",
    "    print(\"... read output size: \"+str(yraw.shape))\n",
    "    # Create complete IO-data\n",
    "    print(\"Pairing X-Y and splitting training/testing data.\")\n",
    "    iotab = pd.merge(yraw, xfiles, on='date', sort=True)\n",
    "    print(\"... data size after merging: \"+str(iotab.shape))\n",
    "    # Dro NA if specified\n",
    "    if dropna:\n",
    "        print('Dropping records with NA')\n",
    "        iotab = iotab.dropna()\n",
    "        print(\"... data size after dropping-NAs: \"+str(iotab.shape))\n",
    "    # Generate weited sampling\n",
    "\n",
    "    # Done\n",
    "    return(iotab)\n",
    "\n",
    "def generate_equal_samples(iotab, prec_bins, ylab='y', shuffle=True):\n",
    "    '''Create equal sampling list: \n",
    "           repeat sample rare categories to mtach the frequency of the majority case.\n",
    "    '''\n",
    "    # Analysis the Precipitation\n",
    "    prec_hist = np.histogram(iotab[ylab], bins=prec_bins)\n",
    "    maxc = np.max(prec_hist[0])                     # Count the most frequent category\n",
    "    nrep = np.round(maxc/prec_hist[0]).astype(int)  # Multiples required to reach max-count\n",
    "    # Categorize precipitation by specified bins\n",
    "    iotab['prec_cat'] = np.digitize(iotab[ylab], bins=prec_bins[1:-1])\n",
    "    logging.debug('Sample histogram before weighted sampling:')\n",
    "    logging.debug(iotab['prec_cat'].value_counts())\n",
    "    # Repeat sampling by p\n",
    "    for icat in range(0,len(prec_bins)-1):\n",
    "        repeat_n = nrep[icat]\n",
    "        tmp = iotab.loc[iotab['prec_cat']==icat,:]\n",
    "        print('Append data category: '+str(icat)+' for '+ str(repeat_n) +' times with size '+str(tmp.shape))\n",
    "        for j in range(int(repeat_n)-1):\n",
    "            iotab = iotab.append(tmp, ignore_index=True)\n",
    "    logging.debug('Sample histogram after weighted sampling:')\n",
    "    logging.debug(iotab['prec_cat'].value_counts().sort_index())\n",
    "    # Shuffle new dataset if specified\n",
    "    if shuffle:\n",
    "        iotab = iotab.sample(frac=1)#.reset_index(drop=True)\n",
    "    #\n",
    "    return(iotab)\n",
    "\n",
    "def loadDBZ(flist):\n",
    "    ''' Load a list a dbz files (in npy format) into one numpy array. '''\n",
    "    xdata = []\n",
    "    for f in flist:\n",
    "        tmp = np.load(f)\n",
    "        xdata.append(tmp)\n",
    "    x = np.array(xdata, dtype=np.float32)\n",
    "    return(x)\n",
    "\n",
    "# Function to give report\n",
    "def report_evaluation(y_true, y_pred, verbose=0):\n",
    "    import sklearn.metrics as metrics\n",
    "    # Calculate measures\n",
    "    results = {}\n",
    "    results['y_true_mean'] = y_true.mean()\n",
    "    results['y_true_var'] = y_true.var()\n",
    "    results['y_pred_mean'] = y_pred.mean()\n",
    "    results['y_pred_var'] = y_pred.var()\n",
    "    results['rmse'] = np.sqrt(metrics.mean_squared_error(y_true,y_pred))\n",
    "    if y_pred.var()<=10e-8:\n",
    "        results['corr'] = 0\n",
    "    else:\n",
    "        results['corr'] = np.corrcoef(y_true,y_pred)[0,1]\n",
    "    # Print results if verbose > 0\n",
    "    if verbose>0:\n",
    "        if verbose>1:\n",
    "            print('Mean of y_true: ' + str(results['y_true_mean']))\n",
    "            print('Variance of y_true: ' + str(results['y_true_var']))\n",
    "            print('Mean of y_pred: ' + str(results['y_pred_mean']))\n",
    "            print('Variance of y_pred: ' + str(results['y_pred_var']))\n",
    "        print('RMSE: ' + str(results['rmse']))\n",
    "        print('Corr: ' + str(results['corr']))\n",
    "    # Return results\n",
    "    return(results)\n",
    "\n",
    "# Create cross validation splits\n",
    "def create_CV_splits(iotable, k=5, ysegs=None, ylab='y', shuffle=False):\n",
    "    from sklearn.model_selection import StratifiedKFold, KFold\n",
    "    # Index of each fold\n",
    "    cvidx_train = []\n",
    "    cvidx_test = []\n",
    "    # If segmentation of y is not specified, use simple KFold\n",
    "    if ysegs is None:\n",
    "        kf = KFold(n_splits=k, random_state=None, shuffle=shuffle)\n",
    "        for idx_train, idx_test in kf.split(iotable['xuri']):\n",
    "            cvidx_train.append(idx_train)\n",
    "            cvidx_test.append(idx_test)\n",
    "    else:\n",
    "        kf = StratifiedKFold(n_splits=k, random_state=None, shuffle=shuffle)\n",
    "        for idx_train, idx_test in kf.split(iotable['xuri'], np.digitize(iotable[ylab], ysegs)):\n",
    "            cvidx_train.append(idx_train)\n",
    "            cvidx_test.append(idx_test)\n",
    "    return((cvidx_train, cvidx_test))\n",
    "\n",
    "def to_onehot(y, nclasses=5):\n",
    "    ''' Represent the given y vector into one-hot encoding of 5 classes (0,1,2,3,4). '''\n",
    "    L = len(y)                                          # Length of vector y\n",
    "    yoh = np.zeros((L, nclasses), dtype=np.float32)     # The one-hot encoding, initialized with 0\n",
    "    for i in range(L):\n",
    "        yoh[i, 0:y[i]] = 1                              # Encode the corresponding class\n",
    "        yoh[i, y[i]] = 1                                # Encode the corresponding class\n",
    "    return(yoh)\n",
    "\n",
    "def data_generator_mlc(iotab, batch_size, ylab='y'):\n",
    "    ''' Data generator for batched processing. '''\n",
    "    nSample = len(iotab)\n",
    "    y = np.array(iotab[ylab])\n",
    "    # This line is just to make the generator infinite, keras needs that    \n",
    "    while True:\n",
    "        batch_start = 0\n",
    "        batch_end = batch_size\n",
    "        while batch_start < nSample:\n",
    "            limit = min(batch_end, nSample)\n",
    "            X = loadDBZ(iotab['xuri'][batch_start:limit])\n",
    "            Y = to_onehot(y[batch_start:limit])\n",
    "            #print(X.shape)\n",
    "            yield (X,Y) #a tuple with two numpy arrays with batch_size samples     \n",
    "            batch_start += batch_size   \n",
    "            batch_end += batch_size\n",
    "    # End of generator\n",
    "\n",
    "# CNN\n",
    "def init_model_mlc(input_shape):\n",
    "    \"\"\"\n",
    "    :Return: \n",
    "      Newly initialized model (regression).\n",
    "    :param \n",
    "      int input_shape: The number of variables to use as input features.\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # blovk1: CONV -> CONV -> MaxPooling\n",
    "    x = Conv2D(filters=32, kernel_size=(3,3), activation='relu', name='block1_conv1', data_format='channels_first')(inputs)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    x = MaxPooling2D((2,2), name='block1_pool', data_format='channels_first')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    # block2: CONV -> CONV -> MaxPooling\n",
    "    x = Conv2D(64, (3,3), activation='relu', name='block2_conv1',data_format='channels_first')(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    x = Conv2D(64, (3,3), activation='relu', name='block2_conv2',data_format='channels_first')(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    x = MaxPooling2D((2,2), name='block2_pool', data_format='channels_first')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    # block3: CONV -> CONV -> MaxPooling\n",
    "    x = Conv2D(128, (3,3), activation='relu', name='block3_conv1',data_format='channels_first')(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    x = Conv2D(128, (3,3), activation='relu', name='block3_conv2',data_format='channels_first')(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    x = MaxPooling2D((2,2), name='block3_pool', data_format='channels_first')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    # Output block: Flatten -> Dense -> Dense -> softmax output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu', name='fc1')(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(16, activation='relu', name='fc2')(x)\n",
    "    x = BatchNormalization(axis=1)(x)\n",
    "    # Output layer\n",
    "    out = Dense(5, activation='sigmoid', name='main_output')(x)\n",
    "    # Initialize model\n",
    "    model = Model(inputs = inputs, outputs = out)\n",
    "    # Define compile parameters\n",
    "    adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    #sgd = SGD(lr=0.01, momentum=1e-8, decay=0.001, nesterov=True)#, clipvalue=1.)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    encoder = Model(inputs = inputs, outputs = x)\n",
    "    return((model, encoder))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test one-hot encoding\n",
    "\n",
    "For multi-label classification, we need to convert continuous y to categorical and one-hot encoding. Let's test it before we proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35064, 45)\n",
      "         date  t1hr\n",
      "0  2013010101   0.0\n",
      "1  2013010102   0.0\n",
      "2  2013010103   0.0\n",
      "3  2013010104   0.0\n",
      "4  2013010105   0.0\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "tmp = pd.read_csv('./data/t1hr.csv')\n",
    "# Move date to index\n",
    "dates = tmp['date']\n",
    "t1hr = tmp.iloc[:,1:]\n",
    "print(t1hr.shape)\n",
    "# Max of stations\n",
    "t1hr_max = t1hr.max(axis=1)\n",
    "\n",
    "# Create y\n",
    "y = pd.DataFrame({'date':dates, 't1hr':t1hr_max})\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0.5, 10, 15, 30, 1000]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([19442., 13428.,   860.,   887.,   447.]),\n",
       " array([0.0e+00, 5.0e-01, 1.0e+01, 1.5e+01, 3.0e+01, 1.0e+03]),\n",
       " <a list of 5 Patch objects>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram\n",
    "print(prec_bins)\n",
    "plt.hist(y.t1hr, bins=prec_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    19442\n",
      "1    13428\n",
      "2      860\n",
      "3      887\n",
      "4      447\n",
      "Name: ycat, dtype: int64\n",
      "4      447\n",
      "3     1334\n",
      "2     2194\n",
      "1    15622\n",
      "0    35064\n",
      "Name: ycat, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Categorization\n",
    "y['ycat'] = np.digitize(y.t1hr, bins=prec_bins[1:-1])\n",
    "print(pd.Series(y.ycat).value_counts().sort_index())\n",
    "print(np.cumsum(np.flip(pd.Series(y.ycat).value_counts().sort_index())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 2 2 2 2 1 1 2 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = np.random.randint(1,6,10)\n",
    "print(tmp)\n",
    "to_onehot(tmp-1, nclasses=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35064.0\n",
      "15622.0\n",
      "2194.0\n",
      "1334.0\n",
      "447.0\n"
     ]
    }
   ],
   "source": [
    "yoh = to_onehot(y.ycat, nclasses=5)\n",
    "for i in range(5):\n",
    "    print(np.sum(yoh[:,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far th results seems fine. We use `np.digitize()` to convert Y to categorical, while removing the boundary values in our bins.\n",
    "\n",
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input X from: ../dbz_2016070609/\n",
      "... read input size: (92, 2)\n",
      "Reading output Y from: ./data/1hrmax.csv\n",
      "... read output size: (35064, 2)\n",
      "Pairing X-Y and splitting training/testing data.\n",
      "... data size after merging: (92, 3)\n",
      "Dropping records with NA\n",
      "... data size after dropping-NAs: (92, 3)\n",
      "Append data category: 0 for 2 times with size (23, 4)\n",
      "Append data category: 1 for 1 times with size (50, 4)\n",
      "Append data category: 2 for 6 times with size (9, 4)\n",
      "Append data category: 3 for 7 times with size (7, 4)\n",
      "Append data category: 4 for 17 times with size (3, 4)\n",
      "(92, 4)\n",
      "(250, 4)\n",
      "0    23\n",
      "1    50\n",
      "2     9\n",
      "3     7\n",
      "4     3\n",
      "Name: prec_cat, dtype: int64\n",
      "0    46\n",
      "1    50\n",
      "2    54\n",
      "3    49\n",
      "4    51\n",
      "Name: prec_cat, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n",
    "srcx = '../dbz_2016070609/'\n",
    "srcy = './data/1hrmax.csv'\n",
    "# Create IO table\n",
    "iotab = loadIOTab(srcx, srcy, dropna=True)\n",
    "# Create weighted sampling rom IOdata\n",
    "newiotab = generate_equal_samples(iotab, prec_bins=prec_bins, ylab='t1hr', shuffle=True)\n",
    "#\n",
    "print(iotab.shape)\n",
    "print(newiotab.shape)\n",
    "print(iotab['prec_cat'].value_counts().sort_index())\n",
    "print(newiotab['prec_cat'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        [(None, 6, 275, 162)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 273, 160)      1760      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 32, 273, 160)      128       \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 136, 80)       0         \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 32, 136, 80)       0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 64, 134, 78)       18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 64, 134, 78)       256       \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 132, 76)       36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 64, 132, 76)       256       \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 64, 66, 38)        0         \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 64, 66, 38)        0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 128, 64, 36)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 128, 64, 36)       512       \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 128, 62, 34)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 128, 62, 34)       512       \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 128, 31, 17)       0         \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 128, 31, 17)       0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 67456)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 256)               17268992  \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 16)                4112      \n",
      "_________________________________________________________________\n",
      "main_output (Dense)          (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 17,554,501\n",
      "Trainable params: 17,553,157\n",
      "Non-trainable params: 1,344\n",
      "_________________________________________________________________\n",
      "       loss  accuracy\n",
      "0  0.386871    0.8200\n",
      "1  0.136955    0.9520\n",
      "2  0.091869    0.9656\n",
      "3  0.068790    0.9768\n",
      "4  0.041693    0.9848\n",
      "5  0.025015    0.9928\n",
      "6  0.060166    0.9784\n",
      "7  0.057768    0.9792\n",
      "8  0.044836    0.9888\n",
      "9  0.015737    0.9960\n"
     ]
    }
   ],
   "source": [
    "model = init_model_mlc((nLayer, nY, nX))\n",
    "model[0].summary()\n",
    "steps_train = np.ceil(newiotab.shape[0]/16)\n",
    "\n",
    "hist = model[0].fit_generator(data_generator_mlc(newiotab, 16, ylab='prec_cat'), steps_per_epoch=steps_train, epochs=10, max_queue_size=16, verbose=0)\n",
    "\n",
    "print(pd.DataFrame(hist.history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 86ms/step\n",
      "[[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "steps_test = np.ceil(iotab.shape[0]/16)\n",
    "y_pred = model[0].predict_generator(data_generator_mlc(iotab, 16, ylab='prec_cat'), steps=steps_test, verbose=1)\n",
    "yp = ((y_pred>0.5)*1.0)\n",
    "print(yp - to_onehot(iotab.prec_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Output\n",
    "\n",
    "We stored another model contains the first N-1 layers of the complete model. Let's check if the partial model is trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of the (n-1)th layer of the complete model:\n",
      "[ 0.19173884 -0.09820265  0.0383967  -0.14314505 -0.11193918 -0.00382341\n",
      " -0.13451558 -0.04491406  0.02882116 -0.018591    0.13941702  0.12168088\n",
      " -0.07752114  0.09162995 -0.02970859  0.17614457]\n",
      "The output of the partial model:\n",
      "[ 0.19173884 -0.09820265  0.0383967  -0.14314505 -0.11193918 -0.00382341\n",
      " -0.13451558 -0.04491406  0.02882116 -0.018591    0.13941702  0.12168088\n",
      " -0.07752114  0.09162995 -0.02970859  0.17614457]\n"
     ]
    }
   ],
   "source": [
    "print('The output of the (n-1)th layer of the complete model:')\n",
    "print(model[0].layers[-2].get_weights()[1])\n",
    "print('The output of the partial model:')\n",
    "print(model[1].layers[-1].get_weights()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
